{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a41bbff2-01df-457f-b79d-140d9ebf8f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 4)\n",
      "        date  sales  stock  price\n",
      "0 2014-01-01      0   4972   1.29\n",
      "1 2014-01-02     70   4902   1.29\n",
      "2 2014-01-03     59   4843   1.29\n",
      "3 2014-01-04     93   4750   1.29\n",
      "4 2014-01-05     96   4654   1.29\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.read_csv(\"dataset/data.csv\")\n",
    "print(df.shape)\n",
    "df.columns = ['date', 'sales', 'stock', 'price']\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Visual check\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95f2a3be-efb9-466f-a196-decf69b5cb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (923, 14, 3), Shape of y: (923,)\n"
     ]
    }
   ],
   "source": [
    "target_col = 'sales'\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df.drop(columns=['date']))  # date not needed for LSTM\n",
    "\n",
    "# Convert to numpy array\n",
    "scaled_data = np.array(scaled_data)\n",
    "\n",
    "# Function to create sequences for LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length, :])      # past seq_length days\n",
    "        y.append(data[i+seq_length, 0])        # 'sales' is the first column after drop(date)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "SEQ_LENGTH = 14  # using past 14 days to predict next day\n",
    "X, y = create_sequences(scaled_data, SEQ_LENGTH)\n",
    "\n",
    "print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f087677-8bf2-4c76-b5f0-d8be6bd9f4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (646, 14, 3), Val: (138, 14, 3), Test: (139, 14, 3)\n"
     ]
    }
   ],
   "source": [
    "# Train: 70%, Validation: 15%, Test: 15%\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.15)\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c67575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bb5bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape, dropout_rate=0.3):\n",
    "    try:\n",
    "        model = Sequential([\n",
    "            InputLayer(shape=input_shape),\n",
    "            \n",
    "            LSTM(32, return_sequences=True, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4), recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_lstm_model: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_gru_model(input_shape, dropout_rate=0.3):\n",
    "    try:\n",
    "        model = Sequential([\n",
    "            InputLayer(shape=input_shape),\n",
    "            \n",
    "            GRU(32, return_sequences=True, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_gru_model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c20fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with callbacks\n",
    "def train_model(model, X_train, y_train, X_val, y_val, model_path, \n",
    "                batch_size=32, epochs=5, patience=15, loss_function='mse', learning_rate=1e-3):\n",
    "    try:\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True),\n",
    "            ModelCheckpoint(model_path, save_best_only=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        # Select the loss function based on the input parameter\n",
    "        if loss_function == 'mse':\n",
    "            loss = 'mean_squared_error'\n",
    "        else:\n",
    "            raise ValueError(\"Invalid loss function specified.\")\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=AdamW(learning_rate=learning_rate),\n",
    "            loss=loss,\n",
    "            metrics=[\n",
    "                'mse', \n",
    "                'mae',\n",
    "                'root_mean_squared_error'\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        return history\n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9577e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function with focus on false negatives\n",
    "def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "    try:\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Calculate various metrics\n",
    "        mse = np.mean((y_test - predictions.flatten())**2)\n",
    "        mae = np.mean(np.abs(y_test - predictions.flatten()))\n",
    "        \n",
    "        # Calculate false negative rate\n",
    "        binary_actual = y_test > threshold\n",
    "        binary_pred = predictions.flatten() > threshold\n",
    "        false_negatives = np.sum((binary_actual == True) & (binary_pred == False))\n",
    "        false_negative_rate = false_negatives / np.sum(binary_actual)\n",
    "        \n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'false_negative_rate': false_negative_rate\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluate_model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d76682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, X_data, y_data, label, start=50, end=500, ylabel='Price', title_suffix=''):\n",
    "    \"\"\"\n",
    "    Plots predictions vs actual values for a given model and dataset.\n",
    "    \n",
    "    Args:\n",
    "    - model: The trained model to use for predictions.\n",
    "    - X_data: Input data for predictions.\n",
    "    - y_data: Actual target values.\n",
    "    - label: A string indicating the dataset (e.g., 'Train', 'Validation', 'Test').\n",
    "    - start, end: Range of data points to visualize (default: 50 to 500).\n",
    "    - ylabel: Label for the y-axis (default: 'Rainfall (mm)').\n",
    "    - title_suffix: Additional suffix for the title (optional).\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame containing the predictions and actual values.\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_data).flatten()\n",
    "\n",
    "    # Create a DataFrame to store results\n",
    "    results_df = pd.DataFrame(data={f'{label} Predictions': predictions, 'Actual Values': y_data})\n",
    "    print(results_df)\n",
    "\n",
    "    # Plot the predictions and actual values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results_df[f'{label} Predictions'][start:end], label=f'{label} Predictions', color='blue', linestyle='-')\n",
    "    plt.plot(results_df['Actual Values'][start:end], label='Actual Values', color='orange', linestyle='--')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Time Stamps', fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.title(f'{label} Predictions vs Actual Values {title_suffix}', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882535f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM model with MAE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 225ms/step - loss: 0.5028 - mae: 0.4908 - mse: 0.5035 - root_mean_squared_error: 0.7096 - val_loss: 0.1371 - val_mae: 0.1252 - val_mse: 0.0309 - val_root_mean_squared_error: 0.1759 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.3091 - mae: 0.2973 - mse: 0.1771 - root_mean_squared_error: 0.4208 - val_loss: 0.1571 - val_mae: 0.1454 - val_mse: 0.0393 - val_root_mean_squared_error: 0.1983 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.2329 - mae: 0.2213 - mse: 0.0977 - root_mean_squared_error: 0.3125 - val_loss: 0.1543 - val_mae: 0.1428 - val_mse: 0.0381 - val_root_mean_squared_error: 0.1952 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.1895 - mae: 0.1781 - mse: 0.0644 - root_mean_squared_error: 0.2538 - val_loss: 0.1489 - val_mae: 0.1376 - val_mse: 0.0360 - val_root_mean_squared_error: 0.1898 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.1634 - mae: 0.1522 - mse: 0.0459 - root_mean_squared_error: 0.2141 - val_loss: 0.1431 - val_mae: 0.1320 - val_mse: 0.0337 - val_root_mean_squared_error: 0.1835 - learning_rate: 0.0010\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 289ms/step\n",
      "Error in evaluate_model: operands could not be broadcast together with shapes (139,) (1946,) \n",
      "Error evaluating LSTM model: operands could not be broadcast together with shapes (139,) (1946,) \n",
      "\n",
      "Training GRU model with MAE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 97ms/step - loss: 0.3904 - mae: 0.3847 - mse: 0.3021 - root_mean_squared_error: 0.5497 - val_loss: 0.1684 - val_mae: 0.1628 - val_mse: 0.0469 - val_root_mean_squared_error: 0.2166 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.2863 - mae: 0.2807 - mse: 0.1594 - root_mean_squared_error: 0.3992 - val_loss: 0.1608 - val_mae: 0.1554 - val_mse: 0.0435 - val_root_mean_squared_error: 0.2085 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.2256 - mae: 0.2202 - mse: 0.0994 - root_mean_squared_error: 0.3152 - val_loss: 0.1526 - val_mae: 0.1473 - val_mse: 0.0400 - val_root_mean_squared_error: 0.2001 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.1895 - mae: 0.1842 - mse: 0.0681 - root_mean_squared_error: 0.2610 - val_loss: 0.1476 - val_mae: 0.1424 - val_mse: 0.0379 - val_root_mean_squared_error: 0.1946 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 0.1673 - mae: 0.1621 - mse: 0.0530 - root_mean_squared_error: 0.2303 - val_loss: 0.1406 - val_mae: 0.1355 - val_mse: 0.0351 - val_root_mean_squared_error: 0.1872 - learning_rate: 0.0010\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 250ms/step\n",
      "Error in evaluate_model: operands could not be broadcast together with shapes (139,) (1946,) \n",
      "Error evaluating GRU model: operands could not be broadcast together with shapes (139,) (1946,) \n",
      "Results for loss function 'mae' saved in '../artifacts/results/cycle_1/test_1/mae'.\n",
      "\n",
      "Training LSTM model with MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 67ms/step - loss: 0.3786 - mae: 0.4318 - mse: 0.3667 - root_mean_squared_error: 0.6055 - val_loss: 0.0576 - val_mae: 0.1616 - val_mse: 0.0457 - val_root_mean_squared_error: 0.2139 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - loss: 0.1675 - mae: 0.2845 - mse: 0.1557 - root_mean_squared_error: 0.3945 - val_loss: 0.0538 - val_mae: 0.1533 - val_mse: 0.0420 - val_root_mean_squared_error: 0.2049 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.1108 - mae: 0.2268 - mse: 0.0991 - root_mean_squared_error: 0.3148 - val_loss: 0.0526 - val_mae: 0.1507 - val_mse: 0.0409 - val_root_mean_squared_error: 0.2023 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0839 - mae: 0.1936 - mse: 0.0723 - root_mean_squared_error: 0.2689 - val_loss: 0.0508 - val_mae: 0.1465 - val_mse: 0.0393 - val_root_mean_squared_error: 0.1982 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0697 - mae: 0.1743 - mse: 0.0582 - root_mean_squared_error: 0.2412 - val_loss: 0.0503 - val_mae: 0.1455 - val_mse: 0.0389 - val_root_mean_squared_error: 0.1972 - learning_rate: 0.0010\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000025AE784A160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 463ms/stepWARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000025AE784A160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step\n",
      "Error in evaluate_model: operands could not be broadcast together with shapes (139,) (1946,) \n",
      "Error evaluating LSTM model: operands could not be broadcast together with shapes (139,) (1946,) \n",
      "\n",
      "Training GRU model with MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 92ms/step - loss: 0.3990 - mae: 0.4228 - mse: 0.3933 - root_mean_squared_error: 0.6271 - val_loss: 0.0924 - val_mae: 0.2526 - val_mse: 0.0867 - val_root_mean_squared_error: 0.2945 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.2202 - mae: 0.3223 - mse: 0.2145 - root_mean_squared_error: 0.4632 - val_loss: 0.0642 - val_mae: 0.1906 - val_mse: 0.0586 - val_root_mean_squared_error: 0.2421 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.1436 - mae: 0.2564 - mse: 0.1381 - root_mean_squared_error: 0.3716 - val_loss: 0.0579 - val_mae: 0.1759 - val_mse: 0.0524 - val_root_mean_squared_error: 0.2289 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.1078 - mae: 0.2251 - mse: 0.1023 - root_mean_squared_error: 0.3199 - val_loss: 0.0528 - val_mae: 0.1646 - val_mse: 0.0473 - val_root_mean_squared_error: 0.2176 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 0.0874 - mae: 0.2014 - mse: 0.0820 - root_mean_squared_error: 0.2864 - val_loss: 0.0490 - val_mae: 0.1561 - val_mse: 0.0436 - val_root_mean_squared_error: 0.2089 - learning_rate: 0.0010\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step\n",
      "Error in evaluate_model: operands could not be broadcast together with shapes (139,) (1946,) \n",
      "Error evaluating GRU model: operands could not be broadcast together with shapes (139,) (1946,) \n",
      "Results for loss function 'mse' saved in '../artifacts/results/cycle_1/test_1/mse'.\n",
      "\n",
      "Model Evaluation Results:\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    try:\n",
    "        # Define loss functions to iterate over\n",
    "        loss_functions = ['mse']\n",
    "        # OUT_DIR = 'artifacts/results/cycle_1/test_1/'\n",
    "\n",
    "        for loss_function in loss_functions:\n",
    "            # Create a directory for the current loss function\n",
    "            results_dir = f'artifacts/results/cycle_1/test_1/{loss_function}'\n",
    "            os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "            # Train models\n",
    "            models = {\n",
    "                'lstm': create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                'gru': create_gru_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            }\n",
    "            \n",
    "            results = {}\n",
    "            for name, model in models.items():\n",
    "                print(f\"\\nTraining {name.upper()} model with {loss_function.upper()} loss...\")\n",
    "                \n",
    "                try:\n",
    "                    hist_path = os.path.join(results_dir, f'model_{name}_{loss_function}.keras')\n",
    "                    history = train_model(\n",
    "                        model, X_train, y_train, X_val, y_val,\n",
    "                        hist_path, \n",
    "                        epochs=5, loss_function=loss_function, learning_rate=1e-3\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error training {name.upper()} model: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    results[name] = evaluate_model(model, X_test, y_test)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating {name.upper()} model: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Save training history to CSV\n",
    "                history_df = pd.DataFrame(history.history)\n",
    "                history_df.to_csv(os.path.join(results_dir, f'{name}_history.csv'), index=False)\n",
    "                \n",
    "                # Plot training history\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.plot(history.history['loss'], label='Training Loss')\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                plt.title(f'{name.upper()} Model Training History')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.savefig(os.path.join(results_dir, f'{name}_training_history.png'))\n",
    "                plt.close()  # Close the plot to free memory\n",
    "                \n",
    "                # Save evaluation results to a text file\n",
    "                with open(os.path.join(results_dir, f'{name}_evaluation.txt'), 'w') as f:\n",
    "                    for metric_name, value in results[name].items():\n",
    "                        f.write(f\"{metric_name}: {value:.4f}\\n\")\n",
    "                \n",
    "                # Plot predictions for Train, Val, and Test datasets and save the plots\n",
    "                    for dataset, data, true_values in zip(['Train', 'Val', 'Test'], \n",
    "                                                        [X_train, X_val, X_test], \n",
    "                                                        [y_train, y_val, y_test]):\n",
    "                        plot_predictions(\n",
    "                            model=model, \n",
    "                            X_data=data, \n",
    "                            y_data=true_values, \n",
    "                            label=name + ' ' + dataset, \n",
    "                            start=100, \n",
    "                            end=500\n",
    "                        )\n",
    "                        # plt.savefig(os.path.join(results_dir, f'{name}_{dataset.lower()}_predictions.png'))\n",
    "                        plt.close()  # Close the plot to free memory\n",
    "            \n",
    "            print(f\"Results for loss function '{loss_function}' saved in '{results_dir}'.\")\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        results = main()\n",
    "        print(\"\\nModel Evaluation Results:\")\n",
    "        for model_name, metrics in results.items():\n",
    "            print(f\"\\n{model_name.upper()}:\")\n",
    "            for metric_name, value in metrics.items():\n",
    "                print(f\"{metric_name}: {value:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unhandled error in execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8692156-dbce-4d25-a1fb-91cff5200669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.0255\n",
      "Epoch [2/50], Loss: 0.0160\n",
      "Epoch [3/50], Loss: 0.0155\n",
      "Epoch [4/50], Loss: 0.0150\n",
      "Epoch [5/50], Loss: 0.0152\n",
      "Epoch [6/50], Loss: 0.0146\n",
      "Epoch [7/50], Loss: 0.0138\n",
      "Epoch [8/50], Loss: 0.0133\n",
      "Epoch [9/50], Loss: 0.0133\n",
      "Epoch [10/50], Loss: 0.0129\n",
      "Epoch [11/50], Loss: 0.0124\n",
      "Epoch [12/50], Loss: 0.0122\n",
      "Epoch [13/50], Loss: 0.0172\n",
      "Epoch [14/50], Loss: 0.0119\n",
      "Epoch [15/50], Loss: 0.0120\n",
      "Epoch [16/50], Loss: 0.0118\n",
      "Epoch [17/50], Loss: 0.0113\n",
      "Epoch [18/50], Loss: 0.0113\n",
      "Epoch [19/50], Loss: 0.0115\n",
      "Epoch [20/50], Loss: 0.0120\n",
      "Epoch [21/50], Loss: 0.0110\n",
      "Epoch [22/50], Loss: 0.0109\n",
      "Epoch [23/50], Loss: 0.0107\n",
      "Epoch [24/50], Loss: 0.0107\n",
      "Epoch [25/50], Loss: 0.0106\n",
      "Epoch [26/50], Loss: 0.0111\n",
      "Epoch [27/50], Loss: 0.0125\n",
      "Epoch [28/50], Loss: 0.0108\n",
      "Epoch [29/50], Loss: 0.0106\n",
      "Epoch [30/50], Loss: 0.0105\n",
      "Epoch [31/50], Loss: 0.0107\n",
      "Epoch [32/50], Loss: 0.0104\n",
      "Epoch [33/50], Loss: 0.0103\n",
      "Epoch [34/50], Loss: 0.0105\n",
      "Epoch [35/50], Loss: 0.0104\n",
      "Epoch [36/50], Loss: 0.0103\n",
      "Epoch [37/50], Loss: 0.0102\n",
      "Epoch [38/50], Loss: 0.0111\n",
      "Epoch [39/50], Loss: 0.0105\n",
      "Epoch [40/50], Loss: 0.0114\n",
      "Epoch [41/50], Loss: 0.0106\n",
      "Epoch [42/50], Loss: 0.0102\n",
      "Epoch [43/50], Loss: 0.0103\n",
      "Epoch [44/50], Loss: 0.0103\n",
      "Epoch [45/50], Loss: 0.0101\n",
      "Epoch [46/50], Loss: 0.0106\n",
      "Epoch [47/50], Loss: 0.0106\n",
      "Epoch [48/50], Loss: 0.0103\n",
      "Epoch [49/50], Loss: 0.0102\n",
      "Epoch [50/50], Loss: 0.0101\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ✅ Convert data to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# ✅ Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# ✅ Define LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # Take last time step\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train.shape[2]   # Number of features\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers)\n",
    "\n",
    "# ✅ Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ✅ Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c219552-89fd-4333-a6b3-12719f039889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0984\n",
      "RMSE: 0.1384\n"
     ]
    }
   ],
   "source": [
    "# Put model in eval mode\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        predictions.extend(outputs.numpy())\n",
    "        actuals.extend(y_batch.numpy())\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "actuals = np.array(actuals)\n",
    "\n",
    "# Metrics\n",
    "mae = np.mean(np.abs(predictions - actuals))\n",
    "rmse = np.sqrt(np.mean((predictions - actuals) ** 2))\n",
    "\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf26236e-4415-4db1-8fdb-3a79ba78e790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
